{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (423260, 28, 28) (423260,)\n",
      "Validation set (105820, 28, 28) (105820,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST_dataset.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (423260, 784) (423260, 10)\n",
      "Validation set (105820, 784) (105820, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 regularization (beta = 0.1)\n",
    "\n",
    "$$\\mathcal{L}^{\\prime} = \\mathcal{L} + \\beta \\frac{1}{2} {|| w ||}_{2}^{2}$$\n",
    "Where $\\mathcal{L}$ is the new computed loss with l2 regularization and $\\beta$ is the parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic regression with l2 ($\\beta = 0.1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = 10000\n",
    "beta = 0.1\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Load the training, validation and test data\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # the softmax and cross-entropy . We take the average of this\n",
    "    # cross-entropy across all training examples (the loss)\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    #Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta*tf.nn.l2_loss(weights))\n",
    "  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 801\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "            predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run()\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression with SGD with l2 ($\\beta = 0.1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    #Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta*tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural network with l2 ($\\beta = 0.01$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #Hiddel layer\n",
    "    hidden_nodes = 1024\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    hidden_biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "    \n",
    "    #Initialize the weights\n",
    "    weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # We compute the softmax and cross-entropy and the average of cross entropy (loss)\n",
    "    logits = tf.matmul(hidden_layer, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    # Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta * tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer using gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_relu = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_relu, weights) + biases)\n",
    "    \n",
    "    test_relu = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic model model with l2 ($\\beta = 0.1$) and small training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    #Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta*tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "max_training_batch = 200\n",
    "train_dataset_2 = train_dataset[:max_training_batch, :]\n",
    "train_labels_2 = train_labels[:max_training_batch]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(offset)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #Hiddel layer\n",
    "    hidden_nodes = 1024\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    hidden_biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "    \n",
    "    # Add dropout\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    hidden_layer_dropout = tf.nn.dropout(hidden_layer, keep_prob=keep_prob)\n",
    "    \n",
    "    #Initialize the weights\n",
    "    weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # We compute the softmax and cross-entropy and the average of cross entropy (loss)\n",
    "    logits = tf.matmul(hidden_layer, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    # Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta * tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer using gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_relu = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_relu, weights) + biases)\n",
    "    \n",
    "    test_relu = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "# Dropout probability\n",
    "keep_probability = 0.5\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : keep_probability}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network with 3 hidden layers, l2 regularization and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(x, labels):\n",
    "    fig = plt.figure()\n",
    "    #fig.add_subplot(121)\n",
    "    for data, label in zip(x, labels):\n",
    "        plt.plot(data, label = label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "#l_rate = 0.1\n",
    "\n",
    "h_nodes_1 = 1024\n",
    "h_nodes_2 = 512\n",
    "h_nodes_3 = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Hidden layer1\n",
    "    h_weights_1 = tf.Variable(tf.truncated_normal([image_size*image_size, h_nodes_1]))\n",
    "    h_biases_1 = tf.Variable(tf.zeros([h_nodes_1]))\n",
    "    h_layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, h_weights_1) + h_biases_1)\n",
    "    \n",
    "    # Add dropout to hidden layer 1\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    h_layer_dropout_1 = tf.nn.dropout(h_layer_1, keep_prob=keep_prob)\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    h_weights_2 = tf.Variable(tf.truncated_normal([h_nodes_1, h_nodes_2]))\n",
    "    h_biases_2 = tf.Variable(tf.zeros([h_nodes_2]))\n",
    "    h_layer_2 = tf.nn.relu(tf.matmul(h_layer_dropout_1, h_weights_2) + h_biases_2)\n",
    "    \n",
    "    # Add dropout to hidden layer 2\n",
    "    h_layer_dropout_2 = tf.nn.dropout(h_layer_2, keep_prob=keep_prob)\n",
    "    \n",
    "    # Hidden layer 3\n",
    "    #h_weights_3 = tf.Variable(tf.truncated_normal([h_nodes_2, h_nodes_3]))\n",
    "    #h_biases_3 = tf.Variable(tf.zeros([h_nodes_3]))\n",
    "    #h_layer_3 = tf.nn.relu(tf.matmul(h_layer_dropout_2, h_weights_3) + h_biases_3)\n",
    "    \n",
    "    # Add dropout to hidden layer 3\n",
    "    #h_layer_dropout_3 = tf.nn.dropout(h_layer_3, keep_prob=keep_prob)\n",
    "    \n",
    "    #Initialize the weights\n",
    "    weights = tf.Variable(\n",
    "            tf.truncated_normal([h_nodes_2, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # We compute the softmax and cross-entropy and the average of cross entropy (loss)\n",
    "    logits = tf.matmul(h_layer_dropout_2, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    # Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta * tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer using gradient descent\n",
    "    global_step = tf.Variable(0)  \n",
    "    l_rate = tf.placeholder(\"float\")\n",
    "    learning_rate = tf.train.exponential_decay(l_rate, global_step, 10000, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step= global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_relu_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, h_weights_1) + h_biases_1)\n",
    "    valid_relu_2 = tf.nn.relu(tf.matmul(valid_relu_1, h_weights_2) + h_biases_2)\n",
    "    #valid_relu_3 = tf.nn.relu(tf.matmul(valid_relu_2, h_weights_3) + h_biases_3)    \n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_relu_2, weights) + biases)\n",
    "    \n",
    "    test_relu_1 = tf.nn.relu(tf.matmul(tf_test_dataset, h_weights_1) + h_biases_1)\n",
    "    test_relu_2 = tf.nn.relu(tf.matmul(test_relu_1, h_weights_2) + h_biases_2)\n",
    "    #test_relu_3 = tf.nn.relu(tf.matmul(test_relu_2, h_weights_3) + h_biases_3)\n",
    "    \n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu_2, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 8993.262695\n",
      "Minibatch loss at step 500: 902.607910\n",
      "Minibatch loss at step 1000: 363.971100\n",
      "Minibatch loss at step 1500: 299.781372\n",
      "Minibatch loss at step 2000: 229.966934\n",
      "Minibatch loss at step 2500: 195.815155\n",
      "Minibatch loss at step 3000: 105.357628\n",
      "Minibatch loss at step 3500: 56.329926\n",
      "Minibatch loss at step 4000: 72.645767\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "# Dropout probability\n",
    "keep_probability = 0.45\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "mini_batch_loss_list = []\n",
    "mini_batch_acc = []\n",
    "valid_acc = []\n",
    "test_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                     keep_prob : keep_probability, l_rate : learning_rate}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            #print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "            mini_batch_acc.append(accuracy(predictions, batch_labels))\n",
    "            valid_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "            test_acc.append(accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "plot_acc([mini_batch_acc, valid_acc, test_acc], ['Mini batch accuracy', 'valid accuracy', 'test accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
