{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (423260, 28, 28) (423260,)\n",
      "Validation set (105820, 28, 28) (105820,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST_dataset.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (423260, 784) (423260, 10)\n",
      "Validation set (105820, 784) (105820, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 regularization (beta = 0.1)\n",
    "\n",
    "$$\\mathcal{L}^{\\prime} = \\mathcal{L} + \\beta \\frac{1}{2} {|| w ||}_{2}^{2}$$\n",
    "Where $\\mathcal{L}$ is the new computed loss with l2 regularization and $\\beta$ is the parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic regression with l2 ($\\beta = 0.1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-5b8c38a81fa9>:21: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_subset = 10000\n",
    "beta = 0.1\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Load the training, validation and test data\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # the softmax and cross-entropy . We take the average of this\n",
    "    # cross-entropy across all training examples (the loss)\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    #Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta*tf.nn.l2_loss(weights))\n",
    "  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 320.394928\n",
      "Training accuracy: 8.6%\n",
      "Validation accuracy: 11.9%\n",
      "Loss at step 100: 0.996920\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 80.7%\n",
      "Loss at step 200: 0.984743\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 80.8%\n",
      "Loss at step 300: 0.984008\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 80.8%\n",
      "Loss at step 400: 0.983803\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 80.8%\n",
      "Loss at step 500: 0.983739\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 80.8%\n",
      "Loss at step 600: 0.983719\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 80.8%\n",
      "Loss at step 700: 0.983712\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 80.8%\n",
      "Loss at step 800: 0.983709\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 80.8%\n",
      "Test accuracy: 87.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "            predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run()\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression with SGD with l2 ($\\beta = 0.1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    #Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta*tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 320.057739\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.3%\n",
      "Minibatch loss at step 500: 0.818244\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1000: 1.040043\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 1500: 0.922549\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 2000: 1.050645\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 2500: 1.186211\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 3000: 1.200305\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.7%\n",
      "Test accuracy: 82.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural network with l2 ($\\beta = 0.01$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #Hiddel layer\n",
    "    hidden_nodes = 1024\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    hidden_biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "    \n",
    "    #Initialize the weights\n",
    "    weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # We compute the softmax and cross-entropy and the average of cross entropy (loss)\n",
    "    logits = tf.matmul(hidden_layer, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    # Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta * tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer using gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_relu = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_relu, weights) + biases)\n",
    "    \n",
    "    test_relu = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 354.457550\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 28.7%\n",
      "Minibatch loss at step 500: 18.044912\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 1000: 10.086536\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 1500: 5.836432\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 2000: 18.377510\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 2500: 2.979774\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 3000: 5.853665\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 3500: 3.483919\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 4000: 3.983942\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.7%\n",
      "Test accuracy: 86.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic model model with l2 ($\\beta = 0.1$) and small training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    #Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta*tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0\n",
      "Minibatch loss at step 0: 327.320526\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 8.3%\n",
      "64\n",
      "Minibatch loss at step 500: 0.796388\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 71.0%\n",
      "56\n",
      "Minibatch loss at step 1000: 0.712508\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.1%\n",
      "48\n",
      "Minibatch loss at step 1500: 0.630646\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 71.7%\n",
      "40\n",
      "Minibatch loss at step 2000: 0.623452\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 71.9%\n",
      "32\n",
      "Minibatch loss at step 2500: 0.605281\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 72.3%\n",
      "24\n",
      "Minibatch loss at step 3000: 0.628848\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Test accuracy: 79.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "max_training_batch = 200\n",
    "train_dataset_2 = train_dataset[:max_training_batch, :]\n",
    "train_labels_2 = train_labels[:max_training_batch]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(offset)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mini batch got 100% accuracy (0 bias), but when it is tested on the validation and test dataset, it got 72 and 79 % accuracy respectively. It is obvious that this model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #Hiddel layer\n",
    "    hidden_nodes = 1024\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    hidden_biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "    \n",
    "    # Add dropout\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    hidden_layer_dropout = tf.nn.dropout(hidden_layer, keep_prob=keep_prob)\n",
    "    \n",
    "    #Initialize the weights\n",
    "    weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # We compute the softmax and cross-entropy and the average of cross entropy (loss)\n",
    "    logits = tf.matmul(hidden_layer, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    # Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta * tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer using gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_relu = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_relu, weights) + biases)\n",
    "    \n",
    "    test_relu = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 345.648163\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 34.3%\n",
      "Minibatch loss at step 500: 20.403534\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1000: 8.258595\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1500: 7.499925\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2000: 14.105494\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2500: 8.884617\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 3000: 6.819185\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 3500: 2.406405\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 4000: 4.578821\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 89.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "# Dropout probability\n",
    "keep_probability = 0.5\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : keep_probability}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network with 2 hidden layers, l2 regularization and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(x, step_list, labels):\n",
    "    fig = plt.figure()\n",
    "    #fig.add_subplot(121)\n",
    "    for data, label in zip(x, labels):\n",
    "        plt.plot(step_list, data, label = label)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "#l_rate = 0.1\n",
    "\n",
    "h_nodes_1 = 1024\n",
    "h_nodes_2 = 512\n",
    "h_nodes_3 = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Hidden layer1\n",
    "    h_weights_1 = tf.Variable(tf.truncated_normal([image_size*image_size, h_nodes_1]))\n",
    "    h_biases_1 = tf.Variable(tf.zeros([h_nodes_1]))\n",
    "    h_layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, h_weights_1) + h_biases_1)\n",
    "    \n",
    "    # Add dropout to hidden layer 1\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    h_layer_dropout_1 = tf.nn.dropout(h_layer_1, keep_prob=keep_prob)\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    h_weights_2 = tf.Variable(tf.truncated_normal([h_nodes_1, h_nodes_2]))\n",
    "    h_biases_2 = tf.Variable(tf.zeros([h_nodes_2]))\n",
    "    h_layer_2 = tf.nn.relu(tf.matmul(h_layer_dropout_1, h_weights_2) + h_biases_2)\n",
    "    \n",
    "    # Add dropout to hidden layer 2\n",
    "    h_layer_dropout_2 = tf.nn.dropout(h_layer_2, keep_prob=keep_prob)\n",
    "    \n",
    "    # Hidden layer 3\n",
    "    #h_weights_3 = tf.Variable(tf.truncated_normal([h_nodes_2, h_nodes_3]))\n",
    "    #h_biases_3 = tf.Variable(tf.zeros([h_nodes_3]))\n",
    "    #h_layer_3 = tf.nn.relu(tf.matmul(h_layer_dropout_2, h_weights_3) + h_biases_3)\n",
    "    \n",
    "    # Add dropout to hidden layer 3\n",
    "    #h_layer_dropout_3 = tf.nn.dropout(h_layer_3, keep_prob=keep_prob)\n",
    "    \n",
    "    #Initialize the weights\n",
    "    weights = tf.Variable(\n",
    "            tf.truncated_normal([h_nodes_2, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # We compute the softmax and cross-entropy and the average of cross entropy (loss)\n",
    "    logits = tf.matmul(h_layer_dropout_2, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    # Add l2 regularization\n",
    "    loss = tf.reduce_mean(loss + beta * tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer using gradient descent\n",
    "    global_step = tf.Variable(0)  \n",
    "    l_rate = tf.placeholder(\"float\")\n",
    "    learning_rate = tf.train.exponential_decay(l_rate, global_step, 10000, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step= global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_relu_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, h_weights_1) + h_biases_1)\n",
    "    valid_relu_2 = tf.nn.relu(tf.matmul(valid_relu_1, h_weights_2) + h_biases_2)\n",
    "    #valid_relu_3 = tf.nn.relu(tf.matmul(valid_relu_2, h_weights_3) + h_biases_3)    \n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_relu_2, weights) + biases)\n",
    "    \n",
    "    test_relu_1 = tf.nn.relu(tf.matmul(tf_test_dataset, h_weights_1) + h_biases_1)\n",
    "    test_relu_2 = tf.nn.relu(tf.matmul(test_relu_1, h_weights_2) + h_biases_2)\n",
    "    #test_relu_3 = tf.nn.relu(tf.matmul(test_relu_2, h_weights_3) + h_biases_3)\n",
    "    \n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu_2, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 11441.789062\n",
      "Minibatch loss at step 500: 743.115601\n",
      "Minibatch loss at step 1000: 440.247101\n",
      "Minibatch loss at step 1500: 302.140869\n",
      "Minibatch loss at step 2000: 192.896484\n",
      "Minibatch loss at step 2500: 152.357803\n",
      "Minibatch loss at step 3000: 102.062782\n",
      "Minibatch loss at step 3500: 81.226219\n",
      "Minibatch loss at step 4000: 53.667324\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "# Dropout probability\n",
    "keep_probability = 0.45\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "mini_batch_loss_list = []\n",
    "mini_batch_acc = []\n",
    "valid_acc = []\n",
    "test_acc = []\n",
    "step_list = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                     keep_prob : keep_probability, l_rate : learning_rate}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            #print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "            mini_batch_acc.append(accuracy(predictions, batch_labels))\n",
    "            valid_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "            test_acc.append(accuracy(test_prediction.eval(), test_labels))\n",
    "            step_list.append(step)\n",
    "\n",
    "plot_acc([mini_batch_acc, valid_acc, test_acc], step_list, ['Mini batch accuracy', 'valid accuracy', 'test accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
